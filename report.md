Отчет 

За основу был взят baseline, базовая модель Xception , который дал 92.83%. Добавление слоя BatchNorm снижало точность до 90.81%. 

Уменьшение размера картинки до 128 снижало качество до 84.86%. 

Добавление в голову еще одного плотного слоя в голове, LR=1e-4 дают - 93.4%.

Увеличение размера картинки до image_size = 350 batch_size = 32 дают - 95.13% на kaggle 96.19%. 

Пробуем еще увеличить картинку image_size = 400 batch_size = 25 дают - 94.55% Пожалуй остановимся на картинке 350. 

Сравним оптимизаторы на baseline с image_size = 350 batch_size = 32, Adam - 95,32%  RMSprop - 94,33% SGD - 54.06% 
Adam справляется чуть лучше, SGD требует больше внимания.  Останемся с Adam. 

Попробуем решить задачу с помощью техники управления Learning Rate(Learning Rate.ipynb). Подгрузка модуля CLR c https://github.com/bckenstler/CLR
Первые 6 эпох обнадеживают, точность на трейне растет, точность на валидации достигает трейна, но потом точность на валидации начинает падать,
и  результат всего 80%. Что-то пошло не так?

Добавим параметров в генераторы картинок   -  shear_range=0.1, zoom_range=0.3,rotation_range = 10 , результат на kaggle 96.239.

Изменим голову - возьмем с двуми плотными слоями , и получаем шикарный результат на kaggle 96.838. 

Пробуем к взять в качестве базовой модели EfficientNetB6 , но результат хуже 96,74%. 
Пробуем к взять в качестве базовой модели EfficientNetB7 , batch_size=7, но результат еще хуже 95,46%. 

Пробуем подобрать количество нейронов в плотных слоях с помощью keras-tuner. Попытка с треском проваливается.

Попробуем fine-tuning на Xception (fine-tuning-xception.ipynb) получаем 96,22%

Меняем "голову" - вместо плотных слоев пробуем Conv(64), потом Conv(512) - результат 96,7% и 96,55%

Первые попытки применить fine-tuning на EfficientNet заканчиваются неудачей, обучение головы и ряда слоев дают всего 12% точности.

Учитывая строение этих моделей, их особенности, получается построить модель сначала на  EfficientNetB0(fite-tuning-EfficientNetB0.ipynb) - 93,39%, а потом и на большей сети EfficientNetB7(fine_tuning_EfficientNetB7.ipynb), batch_size= 7. И хоть сразу не удалось  обучить всю модель как хотелось, обучение все размороженной сети не получается из-за недостатка памяти для такой большой сети, результат уже ощутимый - 97,348%

Уменьшим batch_size до 3 и воспользуемся, ранее обученной моделью (Common.ipynb). Разморозим всю сеть и обучим на одной эпохе с оптимизатором SGD - Качество 97,29%, на тесте неверно угаданы 64 модели, оценка на kaggle - 97,513% Лучший результат!!!

Может обучение на нескольких эпохах поможет еще улучшить качество? Пробуем. Качество повышается до 97,55%, на тесте неверно угадано всего 60 моделей, что скажет kaggle? Всего  - 97,498% Чтобы подняться выше, надо использовать другие подходы.
